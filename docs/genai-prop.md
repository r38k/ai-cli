# LLMの推論オプション

[LLMのプロンプトエンジニアリング ―GitHub Copilotを生んだ開発者が教える生成AIアプリケーション開発](https://www.oreilly.co.jp/books/9784814401130/)

「LLMのプロンプトエンジニアリング」を読んでいて，p262に`logprob`の話がありました．
`logprob`はTransformerベースのLLMが次にどのトークンが来るかの確率の対数です．

Q. なぜ確率の対数で表すのか？

> ご指摘の通り、LLMの出力において各トークンの「尤もっともらしさ」を表すのに、確率そのものではなく「確率の対数（log probability、logprob）」が使われるのには、いくつかの重要な理由があります。
> 主な理由は以下の通りです。
> 数値的安定性（Numerical Stability）/アンダーフロー防止:
> LLMは次に来る単語（トークン）の確率を予測します。文章が長くなると、各トークンの出現確率は通常1より小さい値なので、これらの確率をすべて掛け合わせると、非常に小さな値になってしまいます（例: 0.1 * 0.2 * 0.05 * ...）。
> コンピュータで扱える浮動小数点数には精度限界があり、非常に小さな数を何度も掛け合わせると「アンダーフロー（underflow）」が発生し、結果が0になってしまうことがあります。こうなると、異なる文章の尤度を比較できなくなります。
> 対数を取ることで、掛け算を足し算に変換できます（log(a*b) = log(a) + log(b)）。足し算は掛け算よりも数値的に安定しており、アンダーフローを起こしにくくなります。確率 p (0 < p ≤ 1) の対数 log(p) は必ず0以下の負の数になり、p が小さいほど log(p) はより大きな負の数（マイナス無限大に近づく）になります。これにより、非常に小さな確率も区別可能な数値として扱えます。
> 計算効率:
> 多数の値を掛け合わせるよりも、足し合わせる方が計算コストが低い場合があります。
> 情報理論との関連（特に損失関数）:
> 機械学習、特にLLMの訓練では、「交差エントロピー損失（cross-entropy loss）」のような損失関数がよく使われます。これらの損失関数は、予測確率の対数を含む形で定義されることが一般的です。そのため、モデル内部や評価において対数確率を扱うのは自然な流れです。
> 比較の容易さ:
> 対数確率は単調増加関数（底が1より大きい場合）なので、確率の大小関係と対数確率の大小関係は一致します。つまり、p1 > p2 ならば log(p1) > log(p2) です。したがって、対数確率で比較しても問題ありません。
> 引用文にある「logprobが0の場合、モデルがトークンについて確信していることを意味します」というのは、確率が1（100%）のとき log(1) = 0 となることに対応しています。また、「その値がより負の値であるほど、モデルによってトークンが考慮される可能性が低くなります」というのは、確率が0に近づくほど対数確率はマイナス無限大に近づく性質を反映しています。
> このように、確率の対数（logprob）を使用することは、LLMが長いシーケンスの確率を扱い、学習し、評価する上で非常に実用的かつ数学的に便利な方法なのです。
> *Gemini 2.5 Pro 05-06*

つまるところ，推論の計算上都合が良いということのようです．
数学的に対数を取っても結果(確率の高低)が変わらないので，より正確で楽な方を選ぶということです．

LLMのパラメータというと，TemperatureやTopK，TopPなどはよく目にしますが，logprobを目にするケースはなかったので初見でした．
とはいえ，全く新しいものというわけでもありません．
`Temperature = 0`を設定すると，LLMは常に最も確率の高いトークンのみを選択するようになります．
この最も確率の高い(logprobが0に最も近い)トークンを選ぶときにlogprobを使用しています．
TopK, TopPも同様です．

このlogprobを知ったときに，LLMの設定可能なオプションを細かく見たことなかったなと感じました．
実際，SDKのアップデートで設定できるオプションは増えているので，今までなかったものも多くあります．

改めてこのオプションを見ていこうと思います．

## 各SDKの推論オプション

主要なLLM Providerである，OpenAI，Anthropic，Googleの推論オプションを確認してみます．

### OpenAI

確認に使用したパッケージ: `jsr:@openai/openai@^4.102.0`
[https://github.com/openai/openai-node](https://github.com/openai/openai-node)

<details>
<summary>オプションの型定義</summary>

```ts
export interface ChatCompletionCreateParamsBase {
  /**
   * A list of messages comprising the conversation so far. Depending on the
   * [model](https://platform.openai.com/docs/models) you use, different message
   * types (modalities) are supported, like
   * [text](https://platform.openai.com/docs/guides/text-generation),
   * [images](https://platform.openai.com/docs/guides/vision), and
   * [audio](https://platform.openai.com/docs/guides/audio).
   */
  messages: Array<ChatCompletionMessageParam>;

  /**
   * Model ID used to generate the response, like `gpt-4o` or `o3`. OpenAI offers a
   * wide range of models with different capabilities, performance characteristics,
   * and price points. Refer to the
   * [model guide](https://platform.openai.com/docs/models) to browse and compare
   * available models.
   */
  model: (string & {}) | Shared.ChatModel;

  /**
   * Parameters for audio output. Required when audio output is requested with
   * `modalities: ["audio"]`.
   * [Learn more](https://platform.openai.com/docs/guides/audio).
   */
  audio?: ChatCompletionAudioParam | null;

  /**
   * Number between -2.0 and 2.0. Positive values penalize new tokens based on their
   * existing frequency in the text so far, decreasing the model's likelihood to
   * repeat the same line verbatim.
   */
  frequency_penalty?: number | null;

  /**
   * @deprecated Deprecated in favor of `tool_choice`.
   *
   * Controls which (if any) function is called by the model.
   *
   * `none` means the model will not call a function and instead generates a message.
   *
   * `auto` means the model can pick between generating a message or calling a
   * function.
   *
   * Specifying a particular function via `{"name": "my_function"}` forces the model
   * to call that function.
   *
   * `none` is the default when no functions are present. `auto` is the default if
   * functions are present.
   */
  function_call?: 'none' | 'auto' | ChatCompletionFunctionCallOption;

  /**
   * @deprecated Deprecated in favor of `tools`.
   *
   * A list of functions the model may generate JSON inputs for.
   */
  functions?: Array<ChatCompletionCreateParams.Function>;

  /**
   * Modify the likelihood of specified tokens appearing in the completion.
   *
   * Accepts a JSON object that maps tokens (specified by their token ID in the
   * tokenizer) to an associated bias value from -100 to 100. Mathematically, the
   * bias is added to the logits generated by the model prior to sampling. The exact
   * effect will vary per model, but values between -1 and 1 should decrease or
   * increase likelihood of selection; values like -100 or 100 should result in a ban
   * or exclusive selection of the relevant token.
   */
  logit_bias?: Record<string, number> | null;

  /**
   * Whether to return log probabilities of the output tokens or not. If true,
   * returns the log probabilities of each output token returned in the `content` of
   * `message`.
   */
  logprobs?: boolean | null;

  /**
   * An upper bound for the number of tokens that can be generated for a completion,
   * including visible output tokens and
   * [reasoning tokens](https://platform.openai.com/docs/guides/reasoning).
   */
  max_completion_tokens?: number | null;

  /**
   * @deprecated The maximum number of [tokens](/tokenizer) that can be generated in
   * the chat completion. This value can be used to control
   * [costs](https://openai.com/api/pricing/) for text generated via API.
   *
   * This value is now deprecated in favor of `max_completion_tokens`, and is not
   * compatible with
   * [o-series models](https://platform.openai.com/docs/guides/reasoning).
   */
  max_tokens?: number | null;

  /**
   * Set of 16 key-value pairs that can be attached to an object. This can be useful
   * for storing additional information about the object in a structured format, and
   * querying for objects via API or the dashboard.
   *
   * Keys are strings with a maximum length of 64 characters. Values are strings with
   * a maximum length of 512 characters.
   */
  metadata?: Shared.Metadata | null;

  /**
   * Output types that you would like the model to generate. Most models are capable
   * of generating text, which is the default:
   *
   * `["text"]`
   *
   * The `gpt-4o-audio-preview` model can also be used to
   * [generate audio](https://platform.openai.com/docs/guides/audio). To request that
   * this model generate both text and audio responses, you can use:
   *
   * `["text", "audio"]`
   */
  modalities?: Array<'text' | 'audio'> | null;

  /**
   * How many chat completion choices to generate for each input message. Note that
   * you will be charged based on the number of generated tokens across all of the
   * choices. Keep `n` as `1` to minimize costs.
   */
  n?: number | null;

  /**
   * Whether to enable
   * [parallel function calling](https://platform.openai.com/docs/guides/function-calling#configuring-parallel-function-calling)
   * during tool use.
   */
  parallel_tool_calls?: boolean;

  /**
   * Static predicted output content, such as the content of a text file that is
   * being regenerated.
   */
  prediction?: ChatCompletionPredictionContent | null;

  /**
   * Number between -2.0 and 2.0. Positive values penalize new tokens based on
   * whether they appear in the text so far, increasing the model's likelihood to
   * talk about new topics.
   */
  presence_penalty?: number | null;

  /**
   * **o-series models only**
   *
   * Constrains effort on reasoning for
   * [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently
   * supported values are `low`, `medium`, and `high`. Reducing reasoning effort can
   * result in faster responses and fewer tokens used on reasoning in a response.
   */
  reasoning_effort?: Shared.ReasoningEffort | null;

  /**
   * An object specifying the format that the model must output.
   *
   * Setting to `{ "type": "json_schema", "json_schema": {...} }` enables Structured
   * Outputs which ensures the model will match your supplied JSON schema. Learn more
   * in the
   * [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
   *
   * Setting to `{ "type": "json_object" }` enables the older JSON mode, which
   * ensures the message the model generates is valid JSON. Using `json_schema` is
   * preferred for models that support it.
   */
  response_format?:
    | Shared.ResponseFormatText
    | Shared.ResponseFormatJSONSchema
    | Shared.ResponseFormatJSONObject;

  /**
   * This feature is in Beta. If specified, our system will make a best effort to
   * sample deterministically, such that repeated requests with the same `seed` and
   * parameters should return the same result. Determinism is not guaranteed, and you
   * should refer to the `system_fingerprint` response parameter to monitor changes
   * in the backend.
   */
  seed?: number | null;

  /**
   * Specifies the latency tier to use for processing the request. This parameter is
   * relevant for customers subscribed to the scale tier service:
   *
   * - If set to 'auto', and the Project is Scale tier enabled, the system will
   *   utilize scale tier credits until they are exhausted.
   * - If set to 'auto', and the Project is not Scale tier enabled, the request will
   *   be processed using the default service tier with a lower uptime SLA and no
   *   latency guarentee.
   * - If set to 'default', the request will be processed using the default service
   *   tier with a lower uptime SLA and no latency guarentee.
   * - If set to 'flex', the request will be processed with the Flex Processing
   *   service tier.
   *   [Learn more](https://platform.openai.com/docs/guides/flex-processing).
   * - When not set, the default behavior is 'auto'.
   *
   * When this parameter is set, the response body will include the `service_tier`
   * utilized.
   */
  service_tier?: 'auto' | 'default' | 'flex' | null;

  /**
   * Not supported with latest reasoning models `o3` and `o4-mini`.
   *
   * Up to 4 sequences where the API will stop generating further tokens. The
   * returned text will not contain the stop sequence.
   */
  stop?: string | null | Array<string>;

  /**
   * Whether or not to store the output of this chat completion request for use in
   * our [model distillation](https://platform.openai.com/docs/guides/distillation)
   * or [evals](https://platform.openai.com/docs/guides/evals) products.
   */
  store?: boolean | null;

  /**
   * If set to true, the model response data will be streamed to the client as it is
   * generated using
   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format).
   * See the
   * [Streaming section below](https://platform.openai.com/docs/api-reference/chat/streaming)
   * for more information, along with the
   * [streaming responses](https://platform.openai.com/docs/guides/streaming-responses)
   * guide for more information on how to handle the streaming events.
   */
  stream?: boolean | null;

  /**
   * Options for streaming response. Only set this when you set `stream: true`.
   */
  stream_options?: ChatCompletionStreamOptions | null;

  /**
   * What sampling temperature to use, between 0 and 2. Higher values like 0.8 will
   * make the output more random, while lower values like 0.2 will make it more
   * focused and deterministic. We generally recommend altering this or `top_p` but
   * not both.
   */
  temperature?: number | null;

  /**
   * Controls which (if any) tool is called by the model. `none` means the model will
   * not call any tool and instead generates a message. `auto` means the model can
   * pick between generating a message or calling one or more tools. `required` means
   * the model must call one or more tools. Specifying a particular tool via
   * `{"type": "function", "function": {"name": "my_function"}}` forces the model to
   * call that tool.
   *
   * `none` is the default when no tools are present. `auto` is the default if tools
   * are present.
   */
  tool_choice?: ChatCompletionToolChoiceOption;

  /**
   * A list of tools the model may call. Currently, only functions are supported as a
   * tool. Use this to provide a list of functions the model may generate JSON inputs
   * for. A max of 128 functions are supported.
   */
  tools?: Array<ChatCompletionTool>;

  /**
   * An integer between 0 and 20 specifying the number of most likely tokens to
   * return at each token position, each with an associated log probability.
   * `logprobs` must be set to `true` if this parameter is used.
   */
  top_logprobs?: number | null;

  /**
   * An alternative to sampling with temperature, called nucleus sampling, where the
   * model considers the results of the tokens with top_p probability mass. So 0.1
   * means only the tokens comprising the top 10% probability mass are considered.
   *
   * We generally recommend altering this or `temperature` but not both.
   */
  top_p?: number | null;

  /**
   * A unique identifier representing your end-user, which can help OpenAI to monitor
   * and detect abuse.
   * [Learn more](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids).
   */
  user?: string;

  /**
   * This tool searches the web for relevant results to use in a response. Learn more
   * about the
   * [web search tool](https://platform.openai.com/docs/guides/tools-web-search?api-mode=chat).
   */
  web_search_options?: ChatCompletionCreateParams.WebSearchOptions;
}
```

</details>

`@openai/openai`で使用可能な推論オプションは以下の通りです．

* `frequency_penalty`
* `logit_bias`
* `logprobs`
* `max_completion_tokens`
* `max_tokens`
* `metadata`
* `n`
* `parallel_tool_calls`
* `presence_penalty`
* `reasoning_effort`
* `response_format`
* `seed`
* `stop`
* `temperature`
* `top_logprobs`
* `top_p`

### Google

確認に使用したパッケージ: `npm:@google/genai@^1.0.1`
[https://github.com/googleapis/js-genai](https://github.com/googleapis/js-genai)

<details>
<summary>オプションの型定義</summary>

```ts
/** Optional model configuration parameters.

 For more information, see `Content generation parameters
 <https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/content-generation-parameters>`_.
 */
export declare interface GenerateContentConfig {
    /** Used to override HTTP request options. */
    httpOptions?: HttpOptions;
    /** Abort signal which can be used to cancel the request.

     NOTE: AbortSignal is a client-only operation. Using it to cancel an
     operation will not cancel the request in the service. You will still
     be charged usage for any applicable operations.
     */
    abortSignal?: AbortSignal;
    /** Instructions for the model to steer it toward better performance.
     For example, "Answer as concisely as possible" or "Don't use technical
     terms in your response".
     */
    systemInstruction?: ContentUnion;
    /** Value that controls the degree of randomness in token selection.
     Lower temperatures are good for prompts that require a less open-ended or
     creative response, while higher temperatures can lead to more diverse or
     creative results.
     */
    temperature?: number;
    /** Tokens are selected from the most to least probable until the sum
     of their probabilities equals this value. Use a lower value for less
     random responses and a higher value for more random responses.
     */
    topP?: number;
    /** For each token selection step, the ``top_k`` tokens with the
     highest probabilities are sampled. Then tokens are further filtered based
     on ``top_p`` with the final token selected using temperature sampling. Use
     a lower number for less random responses and a higher number for more
     random responses.
     */
    topK?: number;
    /** Number of response variations to return.
     */
    candidateCount?: number;
    /** Maximum number of tokens that can be generated in the response.
     */
    maxOutputTokens?: number;
    /** List of strings that tells the model to stop generating text if one
     of the strings is encountered in the response.
     */
    stopSequences?: string[];
    /** Whether to return the log probabilities of the tokens that were
     chosen by the model at each step.
     */
    responseLogprobs?: boolean;
    /** Number of top candidate tokens to return the log probabilities for
     at each generation step.
     */
    logprobs?: number;
    /** Positive values penalize tokens that already appear in the
     generated text, increasing the probability of generating more diverse
     content.
     */
    presencePenalty?: number;
    /** Positive values penalize tokens that repeatedly appear in the
     generated text, increasing the probability of generating more diverse
     content.
     */
    frequencyPenalty?: number;
    /** When ``seed`` is fixed to a specific number, the model makes a best
     effort to provide the same response for repeated requests. By default, a
     random number is used.
     */
    seed?: number;
    /** Output response mimetype of the generated candidate text.
     Supported mimetype:
     - `text/plain`: (default) Text output.
     - `application/json`: JSON response in the candidates.
     The model needs to be prompted to output the appropriate response type,
     otherwise the behavior is undefined.
     This is a preview feature.
     */
    responseMimeType?: string;
    /** The `Schema` object allows the definition of input and output data types.
     These types can be objects, but also primitives and arrays.
     Represents a select subset of an [OpenAPI 3.0 schema
     object](https://spec.openapis.org/oas/v3.0.3#schema).
     If set, a compatible response_mime_type must also be set.
     Compatible mimetypes: `application/json`: Schema for JSON response.
     */
    responseSchema?: SchemaUnion;
    /** Configuration for model router requests.
     */
    routingConfig?: GenerationConfigRoutingConfig;
    /** Configuration for model selection.
     */
    modelSelectionConfig?: ModelSelectionConfig;
    /** Safety settings in the request to block unsafe content in the
     response.
     */
    safetySettings?: SafetySetting[];
    /** Code that enables the system to interact with external systems to
     perform an action outside of the knowledge and scope of the model.
     */
    tools?: ToolListUnion;
    /** Associates model output to a specific function call.
     */
    toolConfig?: ToolConfig;
    /** Labels with user-defined metadata to break down billed charges. */
    labels?: Record<string, string>;
    /** Resource name of a context cache that can be used in subsequent
     requests.
     */
    cachedContent?: string;
    /** The requested modalities of the response. Represents the set of
     modalities that the model can return.
     */
    responseModalities?: string[];
    /** If specified, the media resolution specified will be used.
     */
    mediaResolution?: MediaResolution;
    /** The speech generation configuration.
     */
    speechConfig?: SpeechConfigUnion;
    /** If enabled, audio timestamp will be included in the request to the
     model.
     */
    audioTimestamp?: boolean;
    /** The configuration for automatic function calling.
     */
    automaticFunctionCalling?: AutomaticFunctionCallingConfig;
    /** The thinking features configuration.
     */
    thinkingConfig?: ThinkingConfig;
}
```

</details>

`@google/genai`で使用可能な推論オプションは以下の通りです．

* `temperature`
* `topP`
* `topK`
* `candidateCount`
* `maxOutputTokens`
* `stopSequences`
* `responseLogprobs`
* `logprobs`
* `presencePenalty`
* `frequencyPenalty`
* `seed`
* `responseMimeType`
* `responseSchema`
* `routingConfig`
* `modelSelectionConfig`
* `safetySettings`
* `toolConfig`
* `labels`
* `responseModalities`
* `thinkingConfig`

### Anthropic

確認に使用したパッケージ: `npm:@anthropic-ai/sdk@^0.52.0`

<details>
<summary>オプションの型定義</summary>

```ts
export interface MessageCreateParamsBase {
    /**
     * The maximum number of tokens to generate before stopping.
     *
     * Note that our models may stop _before_ reaching this maximum. This parameter
     * only specifies the absolute maximum number of tokens to generate.
     *
     * Different models have different maximum values for this parameter. See
     * [models](https://docs.anthropic.com/en/docs/models-overview) for details.
     */
    max_tokens: number;
    /**
     * Input messages.
     *
     * Our models are trained to operate on alternating `user` and `assistant`
     * conversational turns. When creating a new `Message`, you specify the prior
     * conversational turns with the `messages` parameter, and the model then generates
     * the next `Message` in the conversation. Consecutive `user` or `assistant` turns
     * in your request will be combined into a single turn.
     *
     * Each input message must be an object with a `role` and `content`. You can
     * specify a single `user`-role message, or you can include multiple `user` and
     * `assistant` messages.
     *
     * If the final message uses the `assistant` role, the response content will
     * continue immediately from the content in that message. This can be used to
     * constrain part of the model's response.
     *
     * Example with a single `user` message:
     *
     * ```json
     * [{ "role": "user", "content": "Hello, Claude" }]
     * ```
     *
     * Example with multiple conversational turns:
     *
     * ```json
     * [
     *   { "role": "user", "content": "Hello there." },
     *   { "role": "assistant", "content": "Hi, I'm Claude. How can I help you?" },
     *   { "role": "user", "content": "Can you explain LLMs in plain English?" }
     * ]
     * ```
     *
     * Example with a partially-filled response from Claude:
     *
     * ```json
     * [
     *   {
     *     "role": "user",
     *     "content": "What's the Greek name for Sun? (A) Sol (B) Helios (C) Sun"
     *   },
     *   { "role": "assistant", "content": "The best answer is (" }
     * ]
     * ```
     *
     * Each input message `content` may be either a single `string` or an array of
     * content blocks, where each block has a specific `type`. Using a `string` for
     * `content` is shorthand for an array of one content block of type `"text"`. The
     * following input messages are equivalent:
     *
     * ```json
     * { "role": "user", "content": "Hello, Claude" }
     * ```
     *
     * ```json
     * { "role": "user", "content": [{ "type": "text", "text": "Hello, Claude" }] }
     * ```
     *
     * Starting with Claude 3 models, you can also send image content blocks:
     *
     * ```json
     * {
     *   "role": "user",
     *   "content": [
     *     {
     *       "type": "image",
     *       "source": {
     *         "type": "base64",
     *         "media_type": "image/jpeg",
     *         "data": "/9j/4AAQSkZJRg..."
     *       }
     *     },
     *     { "type": "text", "text": "What is in this image?" }
     *   ]
     * }
     * ```
     *
     * We currently support the `base64` source type for images, and the `image/jpeg`,
     * `image/png`, `image/gif`, and `image/webp` media types.
     *
     * See [examples](https://docs.anthropic.com/en/api/messages-examples#vision) for
     * more input examples.
     *
     * Note that if you want to include a
     * [system prompt](https://docs.anthropic.com/en/docs/system-prompts), you can use
     * the top-level `system` parameter — there is no `"system"` role for input
     * messages in the Messages API.
     *
     * There is a limit of 100000 messages in a single request.
     */
    messages: Array<MessageParam>;
    /**
     * The model that will complete your prompt.\n\nSee
     * [models](https://docs.anthropic.com/en/docs/models-overview) for additional
     * details and options.
     */
    model: Model;
    /**
     * An object describing metadata about the request.
     */
    metadata?: Metadata;
    /**
     * Determines whether to use priority capacity (if available) or standard capacity
     * for this request.
     *
     * Anthropic offers different levels of service for your API requests. See
     * [service-tiers](https://docs.anthropic.com/en/api/service-tiers) for details.
     */
    service_tier?: 'auto' | 'standard_only';
    /**
     * Custom text sequences that will cause the model to stop generating.
     *
     * Our models will normally stop when they have naturally completed their turn,
     * which will result in a response `stop_reason` of `"end_turn"`.
     *
     * If you want the model to stop generating when it encounters custom strings of
     * text, you can use the `stop_sequences` parameter. If the model encounters one of
     * the custom sequences, the response `stop_reason` value will be `"stop_sequence"`
     * and the response `stop_sequence` value will contain the matched stop sequence.
     */
    stop_sequences?: Array<string>;
    /**
     * Whether to incrementally stream the response using server-sent events.
     *
     * See [streaming](https://docs.anthropic.com/en/api/messages-streaming) for
     * details.
     */
    stream?: boolean;
    /**
     * System prompt.
     *
     * A system prompt is a way of providing context and instructions to Claude, such
     * as specifying a particular goal or role. See our
     * [guide to system prompts](https://docs.anthropic.com/en/docs/system-prompts).
     */
    system?: string | Array<TextBlockParam>;
    /**
     * Amount of randomness injected into the response.
     *
     * Defaults to `1.0`. Ranges from `0.0` to `1.0`. Use `temperature` closer to `0.0`
     * for analytical / multiple choice, and closer to `1.0` for creative and
     * generative tasks.
     *
     * Note that even with `temperature` of `0.0`, the results will not be fully
     * deterministic.
     */
    temperature?: number;
    /**
     * Configuration for enabling Claude's extended thinking.
     *
     * When enabled, responses include `thinking` content blocks showing Claude's
     * thinking process before the final answer. Requires a minimum budget of 1,024
     * tokens and counts towards your `max_tokens` limit.
     *
     * See
     * [extended thinking](https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking)
     * for details.
     */
    thinking?: ThinkingConfigParam;
    /**
     * How the model should use the provided tools. The model can use a specific tool,
     * any available tool, decide by itself, or not use tools at all.
     */
    tool_choice?: ToolChoice;
    /**
     * Definitions of tools that the model may use.
     *
     * If you include `tools` in your API request, the model may return `tool_use`
     * content blocks that represent the model's use of those tools. You can then run
     * those tools using the tool input generated by the model and then optionally
     * return results back to the model using `tool_result` content blocks.
     *
     * Each tool definition includes:
     *
     * - `name`: Name of the tool.
     * - `description`: Optional, but strongly-recommended description of the tool.
     * - `input_schema`: [JSON schema](https://json-schema.org/draft/2020-12) for the
     *   tool `input` shape that the model will produce in `tool_use` output content
     *   blocks.
     *
     * For example, if you defined `tools` as:
     *
     * ```json
     * [
     *   {
     *     "name": "get_stock_price",
     *     "description": "Get the current stock price for a given ticker symbol.",
     *     "input_schema": {
     *       "type": "object",
     *       "properties": {
     *         "ticker": {
     *           "type": "string",
     *           "description": "The stock ticker symbol, e.g. AAPL for Apple Inc."
     *         }
     *       },
     *       "required": ["ticker"]
     *     }
     *   }
     * ]
     * ```
     *
     * And then asked the model "What's the S&P 500 at today?", the model might produce
     * `tool_use` content blocks in the response like this:
     *
     * ```json
     * [
     *   {
     *     "type": "tool_use",
     *     "id": "toolu_01D7FLrfh4GYq7yT1ULFeyMV",
     *     "name": "get_stock_price",
     *     "input": { "ticker": "^GSPC" }
     *   }
     * ]
     * ```
     *
     * You might then run your `get_stock_price` tool with `{"ticker": "^GSPC"}` as an
     * input, and return the following back to the model in a subsequent `user`
     * message:
     *
     * ```json
     * [
     *   {
     *     "type": "tool_result",
     *     "tool_use_id": "toolu_01D7FLrfh4GYq7yT1ULFeyMV",
     *     "content": "259.75 USD"
     *   }
     * ]
     * ```
     *
     * Tools can be used for workflows that include running client-side tools and
     * functions, or more generally whenever you want the model to produce a particular
     * JSON structure of output.
     *
     * See our [guide](https://docs.anthropic.com/en/docs/tool-use) for more details.
     */
    tools?: Array<ToolUnion>;
    /**
     * Only sample from the top K options for each subsequent token.
     *
     * Used to remove "long tail" low probability responses.
     * [Learn more technical details here](https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277).
     *
     * Recommended for advanced use cases only. You usually only need to use
     * `temperature`.
     */
    top_k?: number;
    /**
     * Use nucleus sampling.
     *
     * In nucleus sampling, we compute the cumulative distribution over all the options
     * for each subsequent token in decreasing probability order and cut it off once it
     * reaches a particular probability specified by `top_p`. You should either alter
     * `temperature` or `top_p`, but not both.
     *
     * Recommended for advanced use cases only. You usually only need to use
     * `temperature`.
     */
    top_p?: number;
}
```

</details>

`@anthropic-ai/sdk`で使用可能な推論オプションは以下の通りです．

* `max_tokens`
* `metadata`
* `service_tier`
* `stop_sequences`
* `temperature`
* `thinking`
* `tool_choice`
* `top_k`
* `top_p`



## 各SDKの共通オプション

各SDK毎に使用可能なオプションには違いがあります．

3つのSDKで共通して利用可能なオプションは以下の通りです．

* `temperature`
    * モデルが生成するトークンのランダム性を制御する
    * 0 ~ 2の間で設定可能
    * 0に近いほど確率の高いトークンを選択し，2に近いほどランダムにトークンを選択する
    * 通常は0.7 ~ 1.0 程度
* `top_p`
    * 確率の合計がpを超えるまで，確率の高いトークンを選び，その中から次のトークンを選択する
    * `temperature`の代替手法であり，どちらか一方を設定する
    * 基本的には`temperature`を設定する
* `max_tokens`
    * モデルが生成可能な最大トークン数
    * 出力トークン数がこの値を超えると、出力は途中で停止する
    * 設定可能な最大値はモデルによって異なります
* `stop`
    * モデルがトークンの生成を停止するシーケンス
    * 指定した文字列がトークンの生成中に出現すると、出力を停止する
* `tool_choice`
    * モデルが使用するツールをどのように決定するか
    * `auto`に設定するとモデルが使用するツールを選択する

こうしてみると，意外と共通のオプションはそこまで多くないですね．

## オプションの詳細

各オプションがどのように動作するかを確認します．
今回は自分が使うことの多いGeminiの推論オプションを確認します．

自作したCLIツールでオプションを変えて，出力結果を確認していきます．
モデルは`gemini-2.5-flash-preview-05-20`を使用します．

[https://github.com/r38k/ai-cli](https://github.com/r38k/ai-cli)

* [temperature](#temperature)
* [topP](#topP)
* [topK](#topK)
* [candidateCount](#candidateCount)
* [maxOutputTokens](#maxOutputTokens)
* [stopSequences](#stopSequences)
* [responseLogprobs](#responseLogprobs)
* [logprobs](#logprobs)
* [presencePenalty](#presencePenalty)
* [frequencyPenalty](#frequencyPenalty)
* [seed](#seed)
* [responseMimeType](#responseMimeType)
* [responseSchema](#responseSchema)
* [routingConfig](#routingConfig)
* [modelSelectionConfig](#modelSelectionConfig)
* [safetySettings](#safetySettings)
* [toolConfig](#toolConfig)
* [labels](#labels)
* [responseModalities](#responseModalities)
* [thinkingConfig](#thinkingConfig)

### temperature

### topP

### topK

### candidateCount

### maxOutputTokens
`gemini-2.5-flash-preview-05-20`では出力トークンの上限が`65,536`に設定されている．
よってこの値は1 ~ 65,536の範囲で設定できる．

```sh


```

### stopSequences

### responseLogprobs

### logprobs

### presencePenalty

### frequencyPenalty

### seed

### responseMimeType

### responseSchema

### routingConfig

### modelSelectionConfig

### safetySettings

### toolConfig

### labels

### responseModalities

### thinkingConfig
